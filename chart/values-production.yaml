# Production environment overrides
global:
  domain: eng.in.docutag.app
  imageRegistry: ghcr.io/docutag
  # imageVersion: Set automatically by Pulumi deployment (e.g., "1.0.0")
  # Do not hardcode - version is managed via Pulumi config
  imagePullPolicy: IfNotPresent
  storageClass: do-block-storage

# Image pull secrets for GitHub Container Registry
imagePullSecrets:
  - name: ghcr-secret

# Image registry credentials - set via CI/CD --set flags
imageRegistry:
  createPullSecret: true
  server: ghcr.io
  username: ""  # Set via --set in GitHub Actions
  password: ""  # Set via --set in GitHub Actions

controller:
  replicaCount: 3
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 15
    targetCPUUtilizationPercentage: 60
  resources:
    requests:
      memory: "64Mi"
      cpu: "100m"
    limits:
      memory: "100Mi"
      cpu: "250m"
  env:
    generateMockData: false
    workerConcurrency: 20

scraper:
  replicaCount: 2
  # S3 storage configuration (DigitalOcean Spaces)
  storage:
    s3:
      endpoint: ""  # Empty for DigitalOcean Spaces (uses default endpoint)
      region: sfo3  # DigitalOcean region
      bucket: docutag-scraper-data
      usePathStyle: false  # false for DO Spaces, true for MinIO
      # Credentials provided via Pulumi/external secret management:
      # accessKeyId: ""
      # secretAccessKey: ""
  resources:
    requests:
      memory: "64Mi"
      cpu: "100m"
    limits:
      memory: "100Mi"
      cpu: "250m"

textanalyzer:
  replicaCount: 3
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 12
    targetCPUUtilizationPercentage: 60
  resources:
    requests:
      memory: "64Mi"
      cpu: "100m"
    limits:
      memory: "100Mi"
      cpu: "250m"
  env:
    ollamaModel: "gemma3:12b"
    workerConcurrency: 10

scheduler:
  replicaCount: 2
  resources:
    requests:
      memory: "64Mi"
      cpu: "100m"
    limits:
      memory: "100Mi"
      cpu: "250m"
  env:
    defaultSchedule: "0 0 * * *"

web:
  replicaCount: 3
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 8
  resources:
    requests:
      memory: "128Mi"
      cpu: "100m"
    limits:
      memory: "256Mi"
      cpu: "250m"

redis:
  enabled: true
  image:
    repository: redis
    tag: 8.2.2-alpine
  persistence:
    enabled: true
    size: 5Gi  # Reduced from 20Gi - ephemeral cache data, currently using ~2MB
    storageClass: do-block-storage
  resources:
    requests:
      memory: "250Mi"
      cpu: "250m"
    limits:
      memory: "500Mi"
      cpu: "500m"

# Self-hosted PostgreSQL with HA (saves ~$240/month vs managed)
# Passwords are automatically generated and managed by Pulumi (stored encrypted in Pulumi state)
# Do not set passwords here - they will be overridden by Pulumi deployment
postgresql:
  enabled: true
  image:
    repository: postgres
    tag: 14.19-alpine3.21
  database: docutag
  username: docutag
  # password: Set automatically by Pulumi with secure random generation

  persistence:
    enabled: true
    size: 20Gi  # Reduced from 100Gi - currently using 51MB, allows 4x 12-month projection
    storageClass: do-block-storage

  resources:
    requests:
      memory: "500Mi"
      cpu: "250m"
    limits:
      memory: "500Mi"
      cpu: "500m"

# Use external managed DB if preferred (disable postgresql above)
externalDatabase:
  enabled: false
  host: ""  # Set via --set or secret
  port: 25060
  database: docutag
  username: doadmin
  password: ""  # Set via --set or secret

observability:
  enabled: true
  prometheus:
    persistence:
      size: 10Gi  # Reduced from 100Gi - 90d retention, currently using 1.3MB, allows 5x projection
    retention: 90d
    resources:
      requests:
        memory: "250Mi"
        cpu: "100m"
      limits:
        memory: "500Mi"
        cpu: "250m"
  grafana:
    persistence:
      size: 5Gi  # Reduced from 20Gi - currently using 40MB, allows 25x growth
    env:
      authAnonymousEnabled: false
      authDisableLoginForm: false
    adminPassword: ""  # Set via --set or secret
    resources:
      requests:
        memory: "128Mi"
        cpu: "200m"
      limits:
        memory: "128Mi"
        cpu: "400m"
  loki:
    persistence:
      size: 10Gi  # Reduced from 100Gi - currently using 136KB, allows 3x 12-month projection
    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "1Gi"
        cpu: "400m"
  tempo:
    persistence:
      size: 5Gi  # Reduced from 50Gi - allows 5x 12-month projection for trace data
    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "1Gi"
        cpu: "400m"

ingress:
  enabled: true
  tls:
    enabled: true
    certResolver: ""  # Disabled - using cert-manager instead
    certManager:
      enabled: true
      clusterIssuer: letsencrypt-prod

# Progressive delivery with Flagger
# NOTE: Disabled until infrastructure workflow deploys Flagger CRDs
flagger:
  enabled: false  # Enable after infrastructure deployment
  strategy: canary  # Gradual rollout
  analysis:
    interval: 1m
    threshold: 5
    stepWeight: 10  # 10% traffic per step
    iterations: 10  # 10 minutes total rollout
  metrics:
    requestSuccessRate:
      enabled: true
      threshold: 99  # 99% success rate required
    requestDuration:
      enabled: true
      threshold: 500  # p99 latency < 500ms
  webhooks:
    helmTests:
      enabled: true
      timeout: 3m
  alerts:
    slack:
      enabled: false  # Set to true and configure webhookUrl if desired
      webhookUrl: ""  # Set via --set or secret
      channel: "#deployments"

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - docutag
        topologyKey: kubernetes.io/hostname
